---
title: "איך בינה מלאכותית לומדת לנצח במשחקי אסטרטגיה?"
english_slug: how-ai-learns-to-win-in-strategy-games
category: "טכנולוגיה / מדעי המחשב"
tags:
  - בינה מלאכותית
  - למידת חיזוק
  - משחקי אסטרטגיה
  - למידת מכונה
  - AI
---
<h1>איך בינה מלאכותית לומדת לנצח במשחקי אסטרטגיה?</h1>
<p>דמיינו לרגע: מערכת בינה מלאכותית שמתחילה לשחק משחק אסטרטגיה מורכב, כמו שחמט או StarCraft, מבלי שאי פעם שיחקה בו לפני כן. אין לה ידע מוקדם על החוקים, המטרות או האסטרטגיות. ובכל זאת, אחרי תקופה של "אימון" - אלפי או מיליוני משחקים נגד עצמה או נגד שחקנים אחרים - היא הופכת לשחקן על-אנושי, שמסוגל לנצח את גדולי המומחים האנושיים. נשמע כמו מדע בדיוני? זו המציאות בתחום המרתק של למידת חיזוק עמוקה.</p>
<p>אז איך בדיוק "לומדת" בינה מלאכותית לשחק ואף לנצח במשחקים הדורשים אסטרטגיה מורכבת, תכנון לטווח ארוך, והתמודדות עם מידע חלקי בזמן אמת? הכל מתחיל בעקרונות בסיסיים של למידה מניסוי וטעייה, ממש כמו שאנחנו לומדים.</p>

<div id="app-container">
    <h2>המסע של ה-AI: סימולטור למידת חיזוק בסיסי</h2>
    <p>בואו נחקור את הרעיון המרכזי באמצעות סימולציה פשוטה. כאן, סוכן AI ילמד להתמצא במפה קטנה ולמצוא משאב, רק באמצעות ניסוי וטעייה וקבלת "פרס" על הצלחות ו"קנס" על טעויות. זהו מודל בסיסי הממחיש את ליבת למידת החיזוק.</p>

    <div class="controls">
        <div>
            <label for="reward">פרס על מציאת המשאב:</label>
            <input type="number" id="reward" value="100" step="10">
        </div>
        <div>
            <label for="penalty">קנס על כל צעד:</label>
            <input type="number" id="penalty" value="-1" step="0.1">
        </div>
         <div>
            <label for="wall-penalty">קנס על נגיעה בקיר:</label>
            <input type="number" id="wall-penalty" value="-10" step="1">
        </div>
        <div>
            <label for="exploration">סקרנות התחלתית (אפסילון):</label>
            <input type="number" id="exploration" value="0.2" min="0" max="1" step="0.05">
        </div>
        <div>
            <label for="episodes">מספר מחזורי למידה (פרקים):</label>
            <input type="number" id="episodes" value="300" min="50" max="2000" step="50">
        </div>
        <button id="start-sim">התחל מסע למידה</button>
    </div>

    <div class="simulation-area">
        <div id="game-grid">
            <!-- Grid cells will be generated by JS -->
        </div>
        <div class="info-panel">
            <h3>סטטוס למידה</h3>
            <p><strong>מחזור:</strong> <span id="current-episode">0</span> / <span id="total-episodes">0</span></p>
            <p><strong>צעדים במחזור זה:</strong> <span id="steps-in-episode">0</span></p>
            <p><strong>פרס שהושג (מחזור זה):</strong> <span id="episode-reward">0</span></p>
            <p><strong>פרס ממוצע (100 מחזורים אחרונים):</strong> <span id="average-reward">0</span></p>
             <p><strong>רמת סקרנות (אפסילון):</strong> <span id="current-epsilon">0</span></p>
            <p><strong>פעולה אחרונה:</strong> <span id="last-action">-</span></p>
            <div class="legend">
                 <span class="legend-item start">התחלה (S)</span>
                 <span class="legend-item resource">משאב (R)</span>
                 <span class="legend-item agent">סוכן AI</span>
                 <span class="legend-item path">נתיב שעבר בו</span>
             </div>
        </div>
    </div>
     <div class="chart-container">
        <canvas id="reward-chart"></canvas>
    </div>

</div>

<button id="toggle-explanation">הצג הסבר מעמיק</button>

<div id="explanation">
    <h3>מה ראינו בסימולטור?</h3>
    <p>הסימולטור הפשטני ממחיש את ליבת אלגוריתם למידת החיזוק הנקרא **Q-Learning**. סוכן ה-AI למד לנווט ברשת (Environment) מתוך מטרה למקסם את הפרס הכולל לאורך זמן. כל "מחזור למידה" (פרק) הוא ניסיון אחד להגיע מההתחלה (S) למשאב (R).</p>
    <ul>
        <li>הסוכן התחיל עם ידע אפסי על המפה או על הפעולות (מעלה, מטה, ימינה, שמאלה).</li>
        <li>בכל צעד, הוא היה ב"מצב" (מיקום על הרשת) ובחר "פעולה".</li>
        <li>לאחר ביצוע הפעולה, הוא עבר ל"מצב" חדש וקיבל "פרס" (שיכול להיות גם קנס שלילי).</li>
        <li>הוא השתמש בפרס זה כדי לעדכן את ה"ידע" שלו (טבלת Q), וללמוד את "ערך" כל פעולה בכל מצב.</li>
        <li>בתחילת הלמידה, הוא היה "סקרן" (אפסילון גבוה) ובחר פעולות רנדומליות לעיתים קרובות (<strong>חקירה - Exploration</strong>) כדי לגלות את הסביבה והפרסים.</li>
        <li>ככל שהתקדם בתהליך הלמידה (מספר הפרקים גדל), רמת הסקרנות ירדה (אפסילון קטן), והוא החל לבחור יותר בפעולות שהניבו את הערך המירבי לפי הידע שצבר (<strong>ניצול ידע - Exploitation</strong>).</li>
        <li>גרף הפרס לאורך הפרקים הראה כיצד הסוכן משפר את הביצועים שלו לאורך זמן, מוצא את המשאב מהר יותר (פחות צעדים) ומקבל פרס כולל גבוה יותר (פחות קנסות על צעדים וקירות).</li>
    </ul>

    <h3>מבוא: מהם משחקי אסטרטגיה ולמה הם אתגר ל-AI?</h3>
    <p>משחקי אסטרטגיה בזמן אמת (Real-Time Strategy - RTS) כמו StarCraft, Age of Empires, או Command & Conquer דורשים קבלת החלטות רבות, לעיתים תכופות, בזמן ששעון המשחק מתקתק. נדרש ניהול מקביל של כלכלה, בניית בסיס, איסוף משאבים, תנועת יחידות, וקרב. עולם המשחק לרוב עצום, המידע זמין באופן חלקי (רק מה שיחידותיך רואות), והפעולות של שחקנים אחרים משפיעות כל הזמן. השילוב של מרחב החלטות עצום, מידע חלקי, וצורך בפעולה מיידית, הופך משחקי RTS לאחד האתגרים המורכבים ביותר עבור מערכות בינה מלאכותית גנריות.</p>

    <h3>למידת חיזוק (Reinforcement Learning): העיקרון שמאחורי הקסם</h3>
    <p>למידת חיזוק היא גישה בלמידת מכונה בה סוכן (Agent) לומד להתנהג בסביבה (Environment) מסוימת על מנת למקסם את ה"פרס" (Reward) הכולל שהוא מקבל. התהליך הוא מעגלי ודומה ללמידה אנושית מניסיון:</p>
    <ol>
        <li>הסוכן נמצא ב"מצב" (State) מסוים של הסביבה (למשל, מיקום השחקן במפה).</li>
        <li>הוא בוחר "פעולה" (Action) מתוך סט פעולות אפשריות (למשל, לזוז למעלה, לתקוף יחידה).</li>
        <li>הוא מבצע את הפעולה, עובר ל"מצב" חדש, ומקבל "פרס" (שיכול להיות חיובי או שלילי - קנס).</li>
        <li>באמצעות הפרס והמעבר למצב החדש, הסוכן מעדכן את הידע שלו על ערך הפעולות השונות במצבים שונים.</li>
    </ol>
    <p>המטרה היא ללמוד "מדיניות" (Policy) - אסטרטגיה או כלל לבחירת הפעולה הטובה ביותר בכל מצב - שתניב את סך הפרסים המקסימלי לטווח הארוך. זהו מודל חזק במיוחד ללמידה במצבים דינמיים ואינטראקטיביים.</p>

    <h3>יישום ב-RTS: הגדרות מורכבות</h3>
    <p>ביישום למידת חיזוק למשחק RTS, ההגדרות הופכות למורכבות הרבה יותר מהסימולטור הפשטני:</p>
    <ul>
        <li><strong>מצב (State):</strong> כולל את כל המידע הזמין לשחקן - מפת העולם (בגילוי חלקי), מיקומי היחידות, סוגיהן, מצבן הבריאותי, כמות המשאבים הזמינים, מצב המחקר הטכנולוגי, סטטוס מבנים, וכו'. תיאור מצב כזה הוא עצום ומורכב.</li>
        <li><strong>פעולה (Action):</strong> מגוון הפעולות האפשריות בכל רגע הוא עצום - הזזת יחידה ספציפית לנקודה ספציפית, בניית סוג מבנה מסוים במיקום מסוים, איסוף משאבים עם יחידה מסוימת, תקיפת יחידת אויב ספציפית, שדרוג טכנולוגי, וכו'.</li>
        <li><strong>פרס (Reward):</strong> הגדרת פונקציית פרס טובה למשחק מורכב היא אתגר בפני עצמו. פרסים ניתנים על השמדת יחידות אויב, בניית מבנים חיוניים, השגת מטרות משנה, והפרס הגדול מכולם הוא ניצחון במשחק. קנסות ניתנים על אובדן יחידות, פעולות לא יעילות, וכו'. הבעיה היא שהפרסים הגדולים (כמו ניצחון) מגיעים רק בסוף משחק ארוך, מה שהופך את הלמידה ליעילה פחות (בעיית Credit Assignment).</li>
    </ul>

    <h3>המעבר מסימולטור פשוט ל-RTS אמיתי: Deep Reinforcement Learning</h3>
    <p>בסימולטור שלנו, מספר המצבים (25 ריבועים) ומספר הפעולות (4 כיוונים) קטן מספיק כדי לאחסן את "ערך ה-Q" לכל זוג מצב-פעולה בטבלה פשוטה (כמו שמומש בקוד). אבל במשחק RTS אמיתי, מספר המצבים האפשריים הוא אסטרונומי (גדול יותר ממספר האטומים ביקום!). אי אפשר לבנות טבלת Q כזו.</p>
    <p>הפתרון מגיע מתחום ה-**Deep Reinforcement Learning (Deep RL)**. במקום טבלת Q, משתמשים ברשתות נוירונים עמוקות כדי <strong>להעריך</strong> את פונקציית הערך (Q) או את המדיניות ישירות מתוך תיאור המצב (למשל, לקבל כקלט "תמונה" של מסך המשחק או ייצוג מורכב יותר של המצב). רשתות נוירונים מצליחות לעשות הכללה ממצבים ספציפיים שראו למצבים חדשים שלא נתקלו בהם בעבר, מה שמאפשר להן להתמודד עם מרחבי מצבים עצומים.</p>

    <h3>דוגמאות מהעולם האמיתי: AlphaStar ואחרים</h3>
    <p>אחד ההישגים הבולטים של Deep RL הוא **AlphaStar** של DeepMind (חברה-בת של גוגל), מערכת AI שהגיעה לרמה על-אנושית במשחק StarCraft II. AlphaStar לא קיבלה הוראות כיצד לשחק, אלא למדה לחלוטין מניסוי וטעייה (בתוספת אימון התחלתי על משחקים אנושיים). היא התמודדה עם האתגרים של מידע חלקי וזמן אמת, ובסופו של דבר ניצחה שחקני StarCraft II מקצוענים ברמה עולמית. דוגמאות נוספות הן AlphaGo (שניצח את אלוף העולם בשחמט מזרחי - Go) ומערכות AI אחרות שניצחו במשחקי אטארי (Atari) או פוקר.</p>

    <h3>הקשר ללמידה אנושית: ניסוי, טעייה ותובנות</h3>
    <p>למידת חיזוק דומה מאוד לאופן שבו בני אדם ובעלי חיים לומדים מיומנויות רבות. תינוק לומד ללכת על ידי ניסויים רבים ונפילות (קנסות) עד שהוא מצליח לעמוד ולצעוד (פרס). ספורטאי לומד מיומנות חדשה על ידי תרגול חוזר, קבלת פידבק מהגוף ומהמאמן, ושיפור הדרגתי. הליבה של למידת חיזוק - למידה מאינטרקציה ופידבק מהסביבה - היא עיקרון יסודי באינטליגנציה, אנושית ומלאכותית כאחד.</p>
</div>

<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script>
    document.addEventListener('DOMContentLoaded', () => {
        const gridElement = document.getElementById('game-grid');
        const rewardInput = document.getElementById('reward');
        const penaltyInput = document.getElementById('penalty');
        const wallPenaltyInput = document.getElementById('wall-penalty');
        const explorationInput = document.getElementById('exploration');
        const episodesInput = document.getElementById('episodes');
        const startButton = document.getElementById('start-sim');
        const currentEpisodeSpan = document.getElementById('current-episode');
        const totalEpisodesSpan = document.getElementById('total-episodes');
        const stepsInEpisodeSpan = document.getElementById('steps-in-episode');
        const episodeRewardSpan = document.getElementById('episode-reward');
        const averageRewardSpan = document.getElementById('average-reward');
        const currentEpsilonSpan = document.getElementById('current-epsilon');
        const lastActionSpan = document.getElementById('last-action');
        const explanationDiv = document.getElementById('explanation');
        const toggleExplanationButton = document.getElementById('toggle-explanation');
        const rewardChartCanvas = document.getElementById('reward-chart');

        const GRID_SIZE = 5;
        const START_POS = { row: 0, col: 0 };
        const RESOURCE_POS = { row: 4, col: 4 };
        const ACTIONS = ['מעלה', 'מטה', 'שמאלה', 'ימינה']; // 0: Up, 1: Down, 2: Left, 3: Right
        const NUM_ACTIONS = ACTIONS.length;
        const NUM_STATES = GRID_SIZE * GRID_SIZE;

        // Q-Learning Parameters
        const LEARNING_RATE = 0.1; // alpha - How much we update Q-value based on the new info
        const DISCOUNT_FACTOR = 0.9; // gamma - How much we value future rewards
        const MAX_STEPS_PER_EPISODE = GRID_SIZE * GRID_SIZE * 3; // Prevent infinite loops

        let qTable = [];
        let episodeRewards = []; // Store reward per episode for charting
        let chart;
        let isSimulationRunning = false; // Flag to prevent multiple runs

        function initializeGrid() {
            gridElement.innerHTML = '';
            gridElement.style.gridTemplateColumns = `repeat(${GRID_SIZE}, 1fr)`; // Use fr for flexible grid
            gridElement.style.gridTemplateRows = `repeat(${GRID_SIZE}, 1fr)`;
            for (let i = 0; i < NUM_STATES; i++) {
                const cell = document.createElement('div');
                cell.classList.add('grid-cell');
                cell.dataset.stateIndex = i; // Store state index
                const row = Math.floor(i / GRID_SIZE);
                const col = i % GRID_SIZE;
                if (row === START_POS.row && col === START_POS.col) {
                    cell.classList.add('start');
                    cell.innerHTML = 'S'; // Use innerHTML for potential icons/more complex content
                }
                if (row === RESOURCE_POS.row && col === RESOURCE_POS.col) {
                    cell.classList.add('resource');
                    cell.innerHTML = 'R';
                }
                gridElement.appendChild(cell);
            }
        }

        function renderAgent(state) {
             // Remove agent class from *all* cells first
             document.querySelectorAll('.grid-cell.agent').forEach(cell => {
                 cell.classList.remove('agent');
                 // Remove any temporary reward/penalty classes
                 cell.classList.remove('reward-positive', 'reward-negative', 'reward-wall');
             });

             // Mark the current cell as visited path (unless it's S or R)
             const cells = gridElement.querySelectorAll('.grid-cell');
             const previousAgentState = document.querySelector('.grid-cell.agent') ? parseInt(document.querySelector('.grid-cell.agent').dataset.stateIndex) : -1;

             cells.forEach((cell, index) => {
                 const row = Math.floor(index / GRID_SIZE);
                 const col = index % GRID_SIZE;
                 if (index === state && !(row === START_POS.row && col === START_POS.col) && !(row === RESOURCE_POS.row && col === RESOURCE_POS.col)) {
                      cell.classList.add('agent'); // Add agent to the new position
                       cell.classList.remove('path'); // Agent is on the cell, not just visited
                 } else if (index === previousAgentState && !(row === START_POS.row && col === START_POS.col) && !(row === RESOURCE_POS.row && col === RESOURCE_POS.col)) {
                      // Mark the cell the agent *just left* as path
                      cell.classList.add('path');
                 } else if (index === state && (row === START_POS.row && col === START_POS.col)) {
                     // Agent is at Start
                     cell.classList.add('agent');
                     cell.classList.remove('path');
                 } else if (index === state && (row === RESOURCE_POS.row && col === RESOURCE_POS.col)) {
                      // Agent is at Resource (end of episode)
                     cell.classList.add('agent');
                      cell.classList.remove('path'); // Don't mark R as path
                 }
             });

            // If the agent is at the start, ensure it's marked correctly
            const currentAgentCell = gridElement.children[state];
             if (currentAgentCell) {
                currentAgentCell.classList.add('agent');
             }
        }

        function visualizeReward(state, reward) {
            const cell = gridElement.children[state];
            if (cell) {
                cell.classList.remove('reward-positive', 'reward-negative', 'reward-wall'); // Clear previous
                if (reward > 0) {
                    cell.classList.add('reward-positive');
                } else if (reward < 0) {
                     // Check if it was a wall penalty specifically
                    const nextStateIfValid = getNextState(state, qTable[state].indexOf(Math.max(...qTable[state]))); // Simplified guess for next state if action was valid
                    if (nextStateIfValid === -1 || reward === parseFloat(wallPenaltyInput.value)) {
                         cell.classList.add('reward-wall'); // Specific class for wall penalty
                    } else {
                        cell.classList.add('reward-negative'); // General penalty
                    }
                }

                // Remove the class after a short delay
                setTimeout(() => {
                    cell.classList.remove('reward-positive', 'reward-negative', 'reward-wall');
                }, 200); // Short visual feedback
            }
        }


        function resetGridPath() {
             // Clear all path and visited markers
             document.querySelectorAll('.grid-cell.path').forEach(cell => cell.classList.remove('path'));
             document.querySelectorAll('.grid-cell.agent').forEach(cell => cell.classList.remove('agent'));
             document.querySelectorAll('.grid-cell').forEach(cell => {
                  cell.classList.remove('reward-positive', 'reward-negative', 'reward-wall');
                  delete cell.dataset.visitedThisEpisode; // Clear internal tracking if used
             });

             // Re-add agent to start
             const startState = getState(START_POS.row, START_POS.col);
             if(startState !== -1) {
                  const startCell = gridElement.children[startState];
                  if (startCell) startCell.classList.add('agent');
             }

             // Ensure S and R are marked correctly
             const startCell = gridElement.children[getState(START_POS.row, START_POS.col)];
             if (startCell) {
                 startCell.classList.add('start');
                 startCell.innerHTML = 'S';
             }
             const resourceCell = gridElement.children[getState(RESOURCE_POS.row, RESOURCE_POS.col)];
             if (resourceCell) {
                 resourceCell.classList.add('resource');
                 resourceCell.innerHTML = 'R';
             }
        }


        function initializeQTable() {
            qTable = Array(NUM_STATES).fill(null).map(() => Array(NUM_ACTIONS).fill(0));
        }

        function getState(row, col) {
            if (row < 0 || row >= GRID_SIZE || col < 0 || col >= GRID_SIZE) {
                return -1; // Indicates invalid state (out of bounds)
            }
            return row * GRID_SIZE + col;
        }

        function getCoords(state) {
            if (state === -1) return { row: -1, col: -1 };
            return { row: Math.floor(state / GRID_SIZE), col: state % GRID_SIZE };
        }

        function getNextPotentialState(currentState, action) {
            const { row, col } = getCoords(currentState);
            let nextRow = row;
            let nextCol = col;

            switch (action) {
                case 0: nextRow--; break; // Up
                case 1: nextRow++; break; // Down
                case 2: nextCol--; break; // Left
                case 3: nextCol++; break; // Right
            }

            return getState(nextRow, nextCol);
        }

        function getReward(currentState, action, nextPotentialState) {
             if (nextPotentialState === -1) {
                 // Action led to a wall
                 return parseFloat(wallPenaltyInput.value);
             }
            const nextCoords = getCoords(nextPotentialState);
            if (nextCoords.row === RESOURCE_POS.row && nextCoords.col === RESOURCE_POS.col) {
                // Reached resource
                return parseFloat(rewardInput.value);
            }
            // Penalty for each step (if not resource and not wall)
            return parseFloat(penaltyInput.value);
        }

        function chooseAction(currentState, epsilon) {
            if (Math.random() < epsilon) {
                // Explore: Choose random action
                return Math.floor(Math.random() * NUM_ACTIONS);
            } else {
                // Exploit: Choose action with max Q-value
                const qValues = qTable[currentState];
                 // Handle ties - choose randomly among actions with max Q
                const maxQ = Math.max(...qValues);
                const bestActions = qValues.map((q, i) => q === maxQ ? i : -1).filter(i => i !== -1);
                return bestActions[Math.floor(Math.random() * bestActions.length)];
            }
        }

         function updateChart(episode, reward) {
            if (!chart) {
                const ctx = rewardChartCanvas.getContext('2d');
                chart = new Chart(ctx, {
                    type: 'line',
                    data: {
                        labels: [],
                        datasets: [{
                            label: 'פרס כולל למחזור',
                            data: [],
                            borderColor: 'rgb(75, 192, 192)',
                            backgroundColor: 'rgba(75, 192, 192, 0.1)',
                            tension: 0.2,
                            fill: true, // Fill area under the curve
                             pointRadius: 3, // Make points slightly larger
                             pointHoverRadius: 5 // Enlarge points on hover
                        }]
                    },
                    options: {
                         responsive: true,
                         maintainAspectRatio: false, // Allow flexible height
                          animation: {
                             duration: 500 // Chart animation duration
                         },
                         scales: {
                            x: {
                                title: {
                                    display: true,
                                    text: 'מספר מחזור למידה',
                                     font: { weight: 'bold' }
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'פרס כולל במחזור',
                                     font: { weight: 'bold' }
                                }
                            }
                        },
                        plugins: {
                            legend: {
                                display: true,
                                position: 'top',
                                labels: {
                                    usePointStyle: true,
                                }
                            },
                            title: {
                                display: true,
                                text: 'התפתחות הפרס לאורך מחזורי הלמידה',
                                 font: { size: 16, weight: 'bold' }
                            },
                            tooltip: { // Improve tooltips
                                mode: 'index',
                                intersect: false,
                            }
                        },
                         hover: {
                             mode: 'nearest',
                             intersect: true
                         }
                    }
                });
            }

            chart.data.labels.push(episode);
            chart.data.datasets[0].data.push(reward);

            // Keep only the last N data points for better visualization if too many episodes
            const maxDataPoints = 500; // Increased max data points
             if (chart.data.labels.length > maxDataPoints) {
                 chart.data.labels.shift();
                 chart.data.datasets[0].data.shift();
             }

            chart.update();
        }


        async function runSimulation() {
            if (isSimulationRunning) return;
            isSimulationRunning = true;
            startButton.disabled = true;
             startButton.textContent = 'הסימולציה פועלת...';

            const totalEpisodes = parseInt(episodesInput.value, 10);
            totalEpisodesSpan.textContent = totalEpisodes;

             // Reset chart data
             if (chart) {
                 chart.destroy();
                 chart = null;
             }
             episodeRewards = [];
             initializeQTable(); // Start with a fresh Q-table


            for (let i = 1; i <= totalEpisodes; i++) {
                currentEpisodeSpan.textContent = i;
                resetGridPath(); // Clear path visualization for the new episode
                let currentState = getState(START_POS.row, START_POS.col);
                let totalEpisodeReward = 0;
                let steps = 0;

                // Epsilon decay: starts high, decreases over episodes
                const initialEpsilon = parseFloat(explorationInput.value);
                 // Linear decay for simplicity, doesn't reach exactly 0 until after last episode
                const epsilon = initialEpsilon * (1 - (i - 1) / totalEpisodes);
                currentEpsilonSpan.textContent = epsilon.toFixed(2);

                 // Initial render of the agent at the start
                 renderAgent(currentState);
                 stepsInEpisodeSpan.textContent = steps;
                 episodeRewardSpan.textContent = totalEpisodeReward.toFixed(1);
                 const episodeStartDelay = totalEpisodes <= 200 ? 400 : (totalEpisodes <= 500 ? 100 : 20);
                 await new Promise(resolve => setTimeout(resolve, episodeStartDelay)); // Pause at start of episode


                while (steps < MAX_STEPS_PER_EPISODE) {
                     // Determine action based on epsilon-greedy policy
                    const action = chooseAction(currentState, epsilon);
                    lastActionSpan.textContent = ACTIONS[action];

                    const nextPotentialState = getNextPotentialState(currentState, action);
                    const reward = getReward(currentState, action, nextPotentialState); // Reward depends on the outcome of the action

                    let actualNextState = nextPotentialState;

                     // Handle wall collision: If hitting a wall (-1), the agent stays in the current state.
                     if (nextPotentialState === -1) {
                         actualNextState = currentState;
                         // The reward is already the wall penalty in this case
                     }

                     // Q-learning update
                     const oldQ = qTable[currentState][action];
                     let maxFutureQ = 0;
                     if (actualNextState !== -1) { // If landed in a valid state
                          // Find the maximum Q-value for the next state
                         maxFutureQ = Math.max(...qTable[actualNextState]);
                     }
                     // Q-Learning Formula: Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))
                     const newQ = oldQ + LEARNING_RATE * (reward + DISCOUNT_FACTOR * maxFutureQ - oldQ);
                     qTable[currentState][action] = newQ;


                    totalEpisodeReward += reward;
                    steps++;

                     // Visualize the reward received from this step
                     visualizeReward(currentState, reward); // Visualize on the cell agent just moved *from* (or is currently on if hit wall)

                    // Update state for the next step
                    currentState = actualNextState;

                    // Render agent at the new position *after* state update
                    renderAgent(currentState);

                    // Update info panel
                    stepsInEpisodeSpan.textContent = steps;
                    episodeRewardSpan.textContent = totalEpisodeReward.toFixed(1);


                     // Animation delay per step
                    const stepDelay = totalEpisodes <= 50 ? 200 : (totalEpisodes <= 200 ? 80 : (totalEpisodes <= 500 ? 20 : 5)); // Faster for more episodes
                    await new Promise(resolve => setTimeout(resolve, stepDelay));

                    // Check for episode end (reached resource or max steps)
                    if (actualNextState !== -1 && getCoords(actualNextState).row === RESOURCE_POS.row && getCoords(actualNextState).col === RESOURCE_POS.col) {
                         // Reached the resource, episode ends successfully
                         visualizeReward(actualNextState, parseFloat(rewardInput.value)); // Visualize final large reward
                         renderAgent(actualNextState); // Ensure agent is rendered on R
                         stepsInEpisodeSpan.textContent = steps; // Final step count
                         episodeRewardSpan.textContent = totalEpisodeReward.toFixed(1); // Final reward
                         await new Promise(resolve => setTimeout(resolve, 500)); // Pause at the resource
                         break; // End the episode
                    }
                }

                 // Episode ended (either reached resource or max steps)
                 // Ensure final state is rendered and stats are updated if loop finished due to max steps
                 renderAgent(currentState);
                 stepsInEpisodeSpan.textContent = steps;
                 episodeRewardSpan.textContent = totalEpisodeReward.toFixed(1);

                 episodeRewards.push(totalEpisodeReward);
                 updateChart(i, totalEpisodeReward);

                 // Calculate and display average reward over the last 100 episodes (or fewer if less than 100)
                const recentRewards = episodeRewards.slice(-100);
                const averageReward = recentRewards.length > 0 ? recentRewards.reduce((sum, r) => sum + r, 0) / recentRewards.length : 0;
                averageRewardSpan.textContent = averageReward.toFixed(1);


                // Pause between episodes, shorter if many episodes
                 const episodeDelay = totalEpisodes <= 100 ? 400 : (totalEpisodes <= 500 ? 150 : 30);
                await new Promise(resolve => setTimeout(resolve, episodeDelay));
            }

            isSimulationRunning = false;
            startButton.disabled = false;
             startButton.textContent = 'התחל מסע למידה';
            alert('מסע הלמידה הסתיים!');
        }

        // Initial setup
        initializeGrid();
        initializeQTable();
        renderAgent(getState(START_POS.row, START_POS.col));


        startButton.addEventListener('click', runSimulation);

        // Toggle explanation visibility
        toggleExplanationButton.addEventListener('click', () => {
            const isHidden = explanationDiv.style.display === 'none' || explanationDiv.style.display === '';
            explanationDiv.style.display = isHidden ? 'block' : 'none';
            toggleExplanationButton.textContent = isHidden ? 'הסתר הסבר מעמיק' : 'הצג הסבר מעמיק';
        });

        // Ensure initial state of explanation is hidden
        explanationDiv.style.display = 'none';

         // Initial chart rendering (empty)
         updateChart(0, 0); // Initialize chart canvas


    });
</script>

<style>
    /* General Styles */
    #app-container {
        font-family: 'Arial', sans-serif;
        line-height: 1.6;
        margin-top: 30px;
        padding: 25px;
        background: linear-gradient(to bottom right, #eef2f7, #d8e3f0); /* Soft gradient background */
        border-radius: 12px;
        direction: rtl;
        text-align: right;
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.1);
        color: #333;
    }

     #app-container h2 {
        color: #0a2c4d; /* Dark blue/grey */
        text-align: center;
        margin-bottom: 20px;
        font-size: 1.8em;
     }

     #app-container p {
         margin-bottom: 15px;
     }


    /* Controls Styling */
    .controls {
        margin-bottom: 30px;
        padding: 20px;
        background-color: #ffffff; /* White background for controls */
        border-radius: 8px;
        display: flex;
        flex-wrap: wrap;
        gap: 20px;
        align-items: flex-end;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.08);
    }

    .controls > div {
        display: flex;
        flex-direction: column;
        flex-grow: 1;
        min-width: 160px; /* Increased minimum width */
    }

    .controls label {
        font-weight: bold;
        margin-bottom: 8px;
        color: #555;
        font-size: 0.95em;
    }

    .controls input[type="number"] {
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 5px;
        width: 100%;
        box-sizing: border-box;
        font-size: 1em;
        transition: border-color 0.3s ease;
    }

     .controls input[type="number"]:focus {
         border-color: #007bff;
         outline: none;
     }

    .controls button {
        padding: 12px 25px;
        background-color: #007bff; /* Primary blue color */
        color: white;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        font-size: 1.1em;
        font-weight: bold;
        transition: background-color 0.3s ease, transform 0.1s ease;
        min-width: 180px; /* Ensure button has decent minimum width */
    }

    .controls button:hover:not(:disabled) {
        background-color: #0056b3;
    }

     .controls button:active:not(:disabled) {
         transform: scale(0.98);
     }

     .controls button:disabled {
         background-color: #cccccc;
         cursor: not-allowed;
     }


    /* Simulation Area Styling */
    .simulation-area {
        display: flex;
        flex-direction: column; /* Stack elements vertically on smaller screens */
        gap: 30px;
        margin-bottom: 30px;
         align-items: center; /* Center items when stacked */
    }

     @media (min-width: 768px) { /* Arrange side-by-side on wider screens */
         .simulation-area {
             flex-direction: row;
              align-items: flex-start; /* Align items to the top when side-by-side */
         }
     }


    #game-grid {
        display: grid;
        /* Sizes will be set by JS */
        grid-template-columns: repeat(5, 50px); /* Slightly larger cells */
        grid-template-rows: repeat(5, 50px);
        gap: 3px; /* Slightly larger gap */
        border: 3px solid #0a2c4d; /* Darker border */
        background-color: #0a2c4d; /* Background color matching border */
        flex-shrink: 0;
        border-radius: 5px;
        overflow: hidden; /* Ensures inner cells respect border-radius */
         box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }

     @media (min-width: 768px) {
         #game-grid {
             grid-template-columns: repeat(5, 60px); /* Larger cells on wider screens */
             grid-template-rows: repeat(5, 60px);
             gap: 4px;
         }
     }


    .grid-cell {
        width: 100%; /* Make cell fill its grid area */
        height: 100%; /* Make cell fill its grid area */
        background-color: #f8f8f8; /* Off-white cell background */
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1em; /* Slightly larger font */
        font-weight: normal;
        color: #333;
        transition: background-color 0.2s ease, box-shadow 0.2s ease;
         border-radius: 3px; /* Small border radius for cells */
    }

    .grid-cell.start {
        background-color: #a0e0a0; /* Soft green */
        font-weight: bold;
         color: #004d00; /* Dark green text */
    }

    .grid-cell.resource {
        background-color: #ffe0a0; /* Soft yellow */
        font-weight: bold;
         color: #4d3300; /* Dark yellow/brown text */
    }

    .grid-cell.agent {
        background-color: #a0a0ff; /* Soft blue */
        font-weight: bold;
        color: #00004d; /* Dark blue text */
        position: relative; /* Needed for potential pseudo-elements or animations */
         /* Pulsing animation */
         animation: pulse-agent 1s infinite alternate;
    }

    @keyframes pulse-agent {
        from { transform: scale(1); box-shadow: 0 0 5px rgba(0,0,255,0.5); }
        to { transform: scale(1.05); box-shadow: 0 0 10px rgba(0,0,255,0.8); }
    }

     .grid-cell.path {
        background-color: #e0e0e0; /* Light gray for visited cells */
        opacity: 0.7;
         transition: background-color 0.5s ease;
     }

     /* Visual feedback for rewards/penalties */
     .grid-cell.reward-positive {
         background-color: #4CAF50 !important; /* Green, important to override others */
          transition: background-color 0.1s ease;
     }
     .grid-cell.reward-negative {
         background-color: #f44336 !important; /* Red */
          transition: background-color 0.1s ease;
     }
      .grid-cell.reward-wall {
         background-color: #ff9800 !important; /* Orange for wall hits */
          transition: background-color 0.1s ease;
     }


    /* Info Panel Styling */
    .info-panel {
        background-color: #fff;
        padding: 20px;
        border-radius: 8px;
        flex-grow: 1;
        min-width: 250px; /* Minimum width */
         box-shadow: 0 2px 5px rgba(0, 0, 0, 0.08);
         font-size: 1em;
    }

     .info-panel h3 {
         margin-top: 0;
         color: #0a2c4d;
         border-bottom: 1px solid #eee;
         padding-bottom: 10px;
         margin-bottom: 15px;
     }

    .info-panel p {
        margin: 10px 0;
         border-bottom: 1px dashed #eee; /* Separator for stats */
         padding-bottom: 8px;
    }
     .info-panel p:last-of-type {
         border-bottom: none;
         padding-bottom: 0;
     }

     .info-panel p strong {
         color: #555;
     }

    .info-panel span {
        font-weight: normal; /* Make the value itself not bold */
         color: #000; /* Ensure value is clearly visible */
    }

     .legend {
         margin-top: 20px;
         border-top: 1px solid #eee;
         padding-top: 15px;
         display: flex;
         flex-wrap: wrap;
         gap: 15px;
         font-size: 0.9em;
     }

     .legend-item {
         display: flex;
         align-items: center;
     }

     .legend-item::before {
         content: '';
         display: inline-block;
         width: 18px;
         height: 18px;
         margin-left: 8px; /* Space between color box and text */
         border-radius: 3px;
         vertical-align: middle;
         border: 1px solid #ccc; /* Add border for clarity */
     }

      .legend-item.start::before { background-color: #a0e0a0; border-color: #004d00;}
      .legend-item.resource::before { background-color: #ffe0a0; border-color: #4d3300;}
      .legend-item.agent::before { background-color: #a0a0ff; border-color: #00004d;}
      .legend-item.path::before { background-color: #e0e0e0; border-color: #ccc;}


    /* Chart Styling */
    .chart-container {
        width: 100%;
        max-width: 800px; /* Increased max width */
        margin: 30px auto 0 auto; /* Center with margin */
        background-color: #fff;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.08);
    }

     #reward-chart {
         max-height: 300px; /* Limit chart height */
     }


    /* Explanation Section Styling */
    #toggle-explanation {
        display: block;
        width: 220px; /* Slightly wider button */
        margin: 30px auto; /* More space */
        padding: 12px;
        background-color: #17a2b8; /* Info blue/teal */
        color: white;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        font-size: 1.1em;
        text-align: center;
        transition: background-color 0.3s ease, transform 0.1s ease;
        font-weight: bold;
    }

    #toggle-explanation:hover {
        background-color: #138496;
    }
     #toggle-explanation:active {
         transform: scale(0.98);
     }


    #explanation {
        margin-top: 20px;
        padding: 30px; /* More padding */
        background-color: #ffffff;
        border-radius: 12px;
        display: none; /* Hidden by default */
        direction: rtl;
        text-align: right;
         box-shadow: 0 6px 12px rgba(0, 0, 0, 0.1);
    }

    #explanation h3 {
        margin-top: 20px;
        margin-bottom: 15px;
        color: #0a2c4d;
        font-size: 1.6em;
        border-bottom: 1px solid #eee;
        padding-bottom: 10px;
    }

     #explanation p {
         margin-bottom: 15px;
         line-height: 1.7;
         color: #444;
     }

     #explanation ul, #explanation ol {
         margin-bottom: 15px;
         padding-right: 20px; /* Indent list items */
         color: #444;
     }
     #explanation li {
         margin-bottom: 8px;
         line-height: 1.6;
     }

</style>
```